{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9b9004-bc75-4d61-8cec-379eedfb2348",
   "metadata": {},
   "source": [
    "# Toxic Comment Detector\n",
    "\n",
    "This repository contains the final project for CSCI 4050U – Machine Learning.\n",
    "The goal is to detect toxic online comments and assign one or more toxicity labels to each comment.\n",
    "\n",
    "Given a raw text comment, the models predict six binary labels:\n",
    "\n",
    "- `toxic`\n",
    "- `severe_toxic`\n",
    "- `obscene`\n",
    "- `threat`\n",
    "- `insult`\n",
    "- `identity_hate`\n",
    "\n",
    "We compare a classical machine-learning baseline to two neural networks, then use the best model in an interactive demo script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a819b-ff81-4dae-ac4b-47c6f79881bc",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "Online platforms are filled with user-generated comments, some of which can be toxic or harmful.\n",
    "This project builds and compares several models for multi-label toxic comment classification:\n",
    "\n",
    "- A classical baseline using bag-of-words style features (TF-IDF) + Logistic Regression.\n",
    "- A simple neural network with word embeddings and an MLP classifier.\n",
    "- A Bi-LSTM neural network that captures word order and context.\n",
    "\n",
    "Each model outputs six binary labels for a given comment, allowing a single comment to be tagged with multiple toxicity types at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0e738-972a-4014-860d-835087195f39",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "This project is designed around the Kaggle “Toxic Comment Classification Challenge” style dataset:\n",
    "\n",
    "- Each row: a user comment (text) + up to 6 binary labels.\n",
    "- Labels match the six classes listed above.\n",
    "- Train/validation/test splits are created from the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d40073-64f9-47e6-ac3a-4c2c2514b8f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Models\n",
    "\n",
    "We compare three main approaches:\n",
    "\n",
    "1. Model 0 – Baseline (TF-IDF + Logistic Regression)\n",
    "    - Vectorizes text into TF-IDF features.\n",
    "    - Trains a One-vs-Rest Logistic Regression classifier for each label.\n",
    "    - Fast to train and serves as a strong, interpretable baseline. \n",
    "<p></p>\n",
    "2. Model 1 – Embedding + Average Pooling + MLP\n",
    "    - Uses a learned embedding layer to map tokens to dense vectors.\n",
    "    - Averages token embeddings over the sequence.\n",
    "    - Feeds the pooled vector into a small MLP (fully-connected network) for prediction.\n",
    "    - Captures more semantic information than TF-IDF while staying simple and efficient.\n",
    "<p></p>\n",
    "3. Model 2 – Embedding + Bi-LSTM (Best Model)\n",
    "    - Uses embeddings followed by a Bi-LSTM to capture word order and context from both directions.\n",
    "    - The final hidden states (or a pooled representation) go through a fully-connected layer for multi-label classification.\n",
    "    - Typically achieves the best performance among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340db8b-6f04-4903-874b-81c9237e6201",
   "metadata": {},
   "source": [
    "# Model 0 - Baseline (TF-IDF + Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a91772-f08f-4d86-8b4d-eba19aa2a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "df = pd.read_csv(\"data/raw/train.csv\")\n",
    "\n",
    "label_columns = [\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\",\n",
    "]\n",
    "\n",
    "texts = df[\"comment_text\"].fillna(\"\").tolist()\n",
    "y = df[label_columns].values\n",
    "\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y[:, 0],\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100_000,      \n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\",\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_texts)  \n",
    "X_val = vectorizer.transform(X_val_texts)\n",
    "\n",
    "base_clf = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "clf = OneVsRestClassifier(base_clf)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_proba = clf.predict_proba(X_val)\n",
    "y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"F1 micro:\", f1_score(y_val, y_pred, average=\"micro\"))\n",
    "print(\"F1 macro:\", f1_score(y_val, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_val, y_pred, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79715592-5472-49a8-8db9-b7a3f93b2572",
   "metadata": {},
   "source": [
    "# Model 1 – Embedding + Average Pooling + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f468cc-c673-4bcf-bf45-4c60ee5777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "DATA_CSV_PATH = os.path.join(\"data\", \"raw\", \"train.csv\")\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LABEL_COLUMNS = [\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\",\n",
    "]\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_CSV_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_CSV_PATH}. \"\n",
    "        f\"Make sure train.csv is at data/raw/train.csv\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "texts = df[\"comment_text\"].fillna(\"\").tolist()\n",
    "y = df[LABEL_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "print(\"Total samples:\", len(texts))\n",
    "print(\"Label matrix shape:\", y.shape)\n",
    "\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = y[:,0],\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(X_train_texts))\n",
    "print(\"Val samples:\", len(X_val_texts))\n",
    "\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Very simple whitespace + punctuation tokenizer.\n",
    "    Lowercases and splits on non-letter characters.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # replace non-letters with space\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    tokens = text.strip().split()\n",
    "    return tokens\n",
    "\n",
    "counter = collections.Counter()\n",
    "for t in X_train_texts:\n",
    "    counter.update(simple_tokenize(t))\n",
    "\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - 2)\n",
    "itos = [\"<pad>\", \"<unk>\"] + [w  for (w,_) in most_common]\n",
    "stoi = {w : i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "\n",
    "def encode_text(text, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Convert raw text to a fixed-length list of token IDs.\n",
    "    Unknown words -> UNK_IDX, pad/truncate to max_len.\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [PAD_IDX] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return np.array(ids, dtype = np.int64)\n",
    "\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = encode_text(self.texts[idx], self.max_len)\n",
    "        label_vec = self.labels[idx]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), \\\n",
    "               torch.tensor(label_vec, dtype=torch.float32)\n",
    "    \n",
    "train_ds = ToxicCommentsDataset(X_train_texts, y_train, max_len = MAX_LEN)\n",
    "val_ds = ToxicCommentsDataset(X_val_texts, y_val, max_len = MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Batches (train):\", len(train_loader))\n",
    "print(\"Batches (val):\", len(val_loader))\n",
    "\n",
    "class ToxicAvgEmbModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.embedding(input_ids)\n",
    "        avg_emb = emb.mean(dim = 1)\n",
    "        x = self.fc1(avg_emb)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "    \n",
    "model = ToxicAvgEmbModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_labels=len(LABEL_COLUMNS),\n",
    "    pad_idx=PAD_IDX,\n",
    ").to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for input_ids, labels in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold = 0.5):\n",
    "    model.eval()\n",
    "    total_loss  = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_pred.append(probs)\n",
    "            all_true.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "\n",
    "    y_true = np.vstack(all_true)\n",
    "    y_scores = np.vstack(all_pred)\n",
    "    y_hat = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    f1_micro = f1_score(y_true, y_hat, average = \"micro\", zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_hat, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return avg_loss, f1_micro, f1_macro, y_hat, y_true\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "for epoch in range(1,EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model,train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, f1_micro, f1_macro, y_true, y_hat = evaluate(\n",
    "        model, val_loader, criterion, DEVICE \n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f} | \"\n",
    "        f\"val loss: {val_loss:.4f} | \"\n",
    "        f\"F1 micro: {f1_micro:.4f} | \"\n",
    "        f\"F1 macro: {f1_macro:.4f}\"\n",
    "    )\n",
    "if f1_micro > best_val_f1:\n",
    "    best_val_f1 = f1_micro\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    save_path = os.path.join(\"saved_models\", \" model1_avgemb.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"  -> New best model saved to {save_path}\")\n",
    "\n",
    "print(\"\\nClassification report (last epoch):\")\n",
    "print(classification_report(\n",
    "    y_true, y_hat, target_names=LABEL_COLUMNS, zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e7721-0bde-40f6-a205-a2acbc1b6b5f",
   "metadata": {},
   "source": [
    "# Model 2 – Embedding + Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778299a-3d6a-4b32-953f-0b3156036464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "DATA_CSV_PATH = os.path.join(\"data\", \"raw\", \"train.csv\")\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LABEL_COLUMNS = [\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\",\n",
    "]\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_CSV_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_CSV_PATH}. \"\n",
    "        f\"Make sure train.csv is at data/raw/train.csv\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "texts = df[\"comment_text\"].fillna(\"\").tolist()\n",
    "y = df[LABEL_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "print(\"Total samples:\", len(texts))\n",
    "print(\"Label matrix shape:\", y.shape)\n",
    "\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = y[:,0],\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(X_train_texts))\n",
    "print(\"Val samples:\", len(X_val_texts))\n",
    "\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Very simple whitespace + punctuation tokenizer.\n",
    "    Lowercases and splits on non-letter characters.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # replace non-letters with space\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    tokens = text.strip().split()\n",
    "    return tokens\n",
    "\n",
    "counter = collections.Counter()\n",
    "for t in X_train_texts:\n",
    "    counter.update(simple_tokenize(t))\n",
    "\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - 2)\n",
    "itos = [\"<pad>\", \"<unk>\"] + [w  for (w,_) in most_common]\n",
    "stoi = {w : i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "\n",
    "def encode_text(text, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Convert raw text to a fixed-length list of token IDs.\n",
    "    Unknown words -> UNK_IDX, pad/truncate to max_len.\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [PAD_IDX] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return np.array(ids, dtype = np.int64)\n",
    "\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = encode_text(self.texts[idx], self.max_len)\n",
    "        label_vec = self.labels[idx]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), \\\n",
    "               torch.tensor(label_vec, dtype=torch.float32)\n",
    "    \n",
    "train_ds = ToxicCommentsDataset(X_train_texts, y_train, max_len = MAX_LEN)\n",
    "val_ds = ToxicCommentsDataset(X_val_texts, y_val, max_len = MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Batches (train):\", len(train_loader))\n",
    "print(\"Batches (val):\", len(val_loader))\n",
    "\n",
    "class ToxicBiLSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            num_labels,\n",
    "            pad_idx = 0,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.embedding(input_ids)\n",
    "        outputs, (h_n, c_n) = self.lstm(emb)\n",
    "        h_forward = h_n[-2,:,:]\n",
    "        h_backward = h_n[-1,:,:]\n",
    "        h_cat = torch.cat([h_forward, h_backward], dim = 1)\n",
    "        x = self.dropout(h_cat)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "    \n",
    "model = ToxicBiLSTMModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embed_dim = EMBED_DIM,\n",
    "    hidden_dim = HIDDEN_DIM,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    num_labels = len(LABEL_COLUMNS),\n",
    "    pad_idx = PAD_IDX, \n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for input_ids, labels in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold = 0.5):\n",
    "    model.eval()\n",
    "    total_loss  = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_pred.append(probs)\n",
    "            all_true.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "\n",
    "    y_true = np.vstack(all_true)\n",
    "    y_scores = np.vstack(all_pred)\n",
    "    y_hat = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    f1_micro = f1_score(y_true, y_hat, average = \"micro\", zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_hat, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return avg_loss, f1_micro, f1_macro, y_hat, y_true\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "for epoch in range(1,EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model,train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, f1_micro, f1_macro, y_true, y_hat = evaluate(\n",
    "        model, val_loader, criterion, DEVICE \n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f} | \"\n",
    "        f\"val loss: {val_loss:.4f} | \"\n",
    "        f\"F1 micro: {f1_micro:.4f} | \"\n",
    "        f\"F1 macro: {f1_macro:.4f}\"\n",
    "    )\n",
    "if f1_micro > best_val_f1:\n",
    "    best_val_f1 = f1_micro\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    save_path = os.path.join(\"saved_models\", \" model2_bilstm.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"  -> New best model saved to {save_path}\")\n",
    "\n",
    "print(\"\\nClassification report (last epoch):\")\n",
    "print(classification_report(\n",
    "    y_true, y_hat, target_names=LABEL_COLUMNS, zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d2017-5b00-440d-a6af-b01afaf0d7c7",
   "metadata": {},
   "source": [
    "# Interactive CLI Demo Using Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460b779-8ffd-4747-91c8-60d435c0ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_CSV_PATH = os.path.join(\"data\", \"raw\", \"train.csv\")\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LABEL_COLUMNS = [\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\",\n",
    "]\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_CSV_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_CSV_PATH}. \"\n",
    "        f\"Make sure train.csv is at data/raw/train.csv\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "texts = df[\"comment_text\"].fillna(\"\").tolist()\n",
    "y = df[LABEL_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "# same split as model2 (so vocab is built on train only)\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y[:, 0],\n",
    ")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Same simple tokenizer as model2.py\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    tokens = text.strip().split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "counter = collections.Counter()\n",
    "for t in X_train_texts:\n",
    "    counter.update(simple_tokenize(t))\n",
    "\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - 2)\n",
    "\n",
    "itos = [\"<pad>\", \"<unk>\"] + [w for (w, _) in most_common]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "vocab_size = len(itos)\n",
    "\n",
    "\n",
    "def encode_text(text, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Convert text to fixed-length list of token IDs.\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [PAD_IDX] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "class ToxicBiLSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        num_labels,\n",
    "        pad_idx=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.embedding(input_ids)     # (batch, seq_len, embed_dim)\n",
    "        outputs, (h_n, c_n) = self.lstm(emb)\n",
    "        # last layer's forward and backward hidden states\n",
    "        h_forward = h_n[-2, :, :]\n",
    "        h_backward = h_n[-1, :, :]\n",
    "        h_cat = torch.cat([h_forward, h_backward], dim=1)\n",
    "        x = self.dropout(h_cat)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "    \n",
    "model_path = os.path.join(\"saved_models\", \" model2_bilstm.pt\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {model_path}. \"\n",
    "        f\"Train model2.py first so it saves the weights.\"\n",
    "    )\n",
    "\n",
    "model = ToxicBiLSTMModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=len(LABEL_COLUMNS),\n",
    "    pad_idx=PAD_IDX,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model2_bilstm.pt\")\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Ready for interactive demo.\\n\")\n",
    "\n",
    "def predict_comment(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Encode text, run model, return label probabilities and predictions.\n",
    "    \"\"\"\n",
    "    ids = encode_text(text, MAX_LEN)\n",
    "    input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(DEVICE)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]  # shape: (6,)\n",
    "\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "    return probs, predictions\n",
    "\n",
    "\n",
    "def pretty_print_predictions(text, probs, preds, threshold=0.5):\n",
    "    print(\"\\nInput comment:\")\n",
    "    print(text)\n",
    "    print(\"\\nPredicted labels (threshold = {:.2f}):\".format(threshold))\n",
    "    for label, p, pred in zip(LABEL_COLUMNS, probs, preds):\n",
    "        status = \"YES\" if pred == 1 else \"no\"\n",
    "        print(f\"  {label:13s}  ->  {status:3s}  (p = {p:.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Type a comment to classify. Type 'quit' to exit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Your comment: \").strip()\n",
    "        if user_input.lower() in {\"quit\", \"exit\"}:\n",
    "            print(\"Exiting demo.\")\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            print(\"Please type something.\\n\")\n",
    "            continue\n",
    "\n",
    "        probs, preds = predict_comment(user_input, threshold=0.5)\n",
    "        pretty_print_predictions(user_input, probs, preds, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
